{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ Advanced RVC Inference Pro\n",
        "\n",
        "**State-of-the-art Voice Conversion with KADVC Optimization**\n",
        "\n",
        "This notebook provides:\n",
        "- ‚ö° **Dependency Caching** - Skip installation on restarts\n",
        "- üóÑÔ∏è **Drive Mounting** - Auto symlink weights folder\n",
        "- üåê **Tunneling** - Robust Gradio/ngrok integration  \n",
        "- üéØ **GPU Auto-detection** - Tesla T4/P100/A100 optimization\n",
        "- üß† **Memory Management** - Automatic OOM prevention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_dependencies"
      },
      "source": [
        "## üì¶ Install & Setup Dependencies\n",
        "\n",
        "This cell includes intelligent caching to save 3-5 minutes on restarts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dependency_install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# ADVANCED RVC DEPENDENCY INSTALLATION\n",
        "# WITH INTELLIGENT CACHING & OPTIMIZATION\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import subprocess\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup paths\n",
        "HOME = Path.home()\n",
        "WORKSPACE = Path.cwd()\n",
        "CACHE_FILE = HOME / \".rvc_dependencies_installed\"\n",
        "TORCH_CACHE = HOME / \".cache/torch\"\n",
        "TRANSFORMERS_CACHE = HOME / \".cache/huggingface\"\n",
        "\n",
        "def check_gpu_type():\n",
        "    \"\"\"Detect GPU type and return optimization recommendations.\"\"\"\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            gpu_name = torch.cuda.get_device_name(0)\n",
        "            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "            \n",
        "            print(f\"üñ•Ô∏è Detected GPU: {gpu_name}\")\n",
        "            print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
        "            \n",
        "            # Auto-configure based on GPU\n",
        "            if \"A100\" in gpu_name:\n",
        "                config = {\"batch_size\": 8, \"precision\": \"fp16\", \"optimization\": \"aggressive\"}\n",
        "                print(\"üöÄ A100 detected - Enabling maximum performance mode\")\n",
        "            elif \"V100\" in gpu_name:\n",
        "                config = {\"batch_size\": 6, \"precision\": \"fp16\", \"optimization\": \"balanced\"}\n",
        "                print(\"‚ö° V100 detected - Enabling balanced performance mode\")\n",
        "            elif \"T4\" in gpu_name:\n",
        "                config = {\"batch_size\": 4, \"precision\": \"fp16\", \"optimization\": \"conservative\"}\n",
        "                print(\"üí™ T4 detected - Enabling conservative performance mode\")\n",
        "            elif \"P100\" in gpu_name:\n",
        "                config = {\"batch_size\": 4, \"precision\": \"fp16\", \"optimization\": \"conservative\"}\n",
        "                print(\"‚ö° P100 detected - Enabling conservative performance mode\")\n",
        "            else:\n",
        "                config = {\"batch_size\": 2, \"precision\": \"fp32\", \"optimization\": \"minimal\"}\n",
        "                print(\"‚ö†Ô∏è Unknown GPU - Using minimal configuration\")\n",
        "            \n",
        "            return config\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No GPU detected - Using CPU mode\")\n",
        "            return {\"batch_size\": 1, \"precision\": \"fp32\", \"optimization\": \"cpu\"}\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error detecting GPU: {e}\")\n",
        "        return {\"batch_size\": 2, \"precision\": \"fp32\", \"optimization\": \"safe\"}\n",
        "\n",
        "def install_pytorch_with_caching():\n",
        "    \"\"\"Install PyTorch with intelligent caching.\"\"\"\n",
        "    print(\"üî• Installing PyTorch with CUDA support...\")\n",
        "    \n",
        "    # Check if PyTorch is already installed\n",
        "    try:\n",
        "        import torch\n",
        "        print(f\"‚úÖ PyTorch {torch.__version__} already installed\")\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"‚úÖ CUDA {torch.version.cuda} available\")\n",
        "            return True\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    # Install PyTorch with CUDA\n",
        "    install_cmd = [\n",
        "        \"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\", \n",
        "        \"--index-url\", \"https://download.pytorch.org/whl/cu118\",\n",
        "        \"--timeout\", \"300\"\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        subprocess.run(install_cmd, check=True, timeout=600)\n",
        "        print(\"‚úÖ PyTorch installed successfully\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Failed to install PyTorch: {e}\")\n",
        "        return False\n",
        "\n",
        "def install_gradio_with_caching():\n",
        "    \"\"\"Install Gradio with latest features.\"\"\"\n",
        "    print(\"üé® Installing Gradio...\")\n",
        "    \n",
        "    try:\n",
        "        import gradio as gr\n",
        "        print(f\"‚úÖ Gradio {gr.__version__} already installed\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        subprocess.run([\"pip\", \"install\", \"gradio\", \"--upgrade\", \"--timeout\", \"120\"], \n",
        "                      check=True, timeout=300)\n",
        "        print(\"‚úÖ Gradio installed successfully\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Failed to install Gradio: {e}\")\n",
        "        return False\n",
        "\n",
        "def install_voice_conversion_deps():\n",
        "    \"\"\"Install voice conversion specific dependencies.\"\"\"\n",
        "    print(\"üé§ Installing Voice Conversion Dependencies...\")\n",
        "    \n",
        "    deps = [\n",
        "        \"librosa\", \"soundfile\", \"audioread\", \"resampy\",\n",
        "        \"sciplot\", \"matplotlib\", \"seaborn\", \"pandas\", \n",
        "        \"numpy\", \"scipy\", \"sklearn\", \"tqdm\",\n",
        "        \"faiss-cpu\", \"onnx\", \"onnxruntime\",\n",
        "        \"ffmpeg-python\", \"youtube-dl\",\n",
        "        \"huggingface-hub\", \"transformers\",\n",
        "        \"face-recognition\", \"dlib\",\n",
        "        \"gdown\", \"wget\", \"requests\"\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        for dep in deps:\n",
        "            print(f\"Installing {dep}...\")\n",
        "            subprocess.run([\"pip\", \"install\", dep, \"--timeout\", \"180\"], \n",
        "                          check=True, timeout=300)\n",
        "        print(\"‚úÖ All voice conversion dependencies installed\")\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Failed to install dependencies: {e}\")\n",
        "        return False\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Setup optimized environment.\"\"\"\n",
        "    print(\"‚öôÔ∏è Setting up optimized environment...\")\n",
        "    \n",
        "    # Create necessary directories\n",
        "    dirs = [\"weights\", \"indexes\", \"logs\", \"cache\", \"temp\", \"audio_files\", \"outputs\"]\n",
        "    for dir_name in dirs:\n",
        "        dir_path = WORKSPACE / dir_name\n",
        "        dir_path.mkdir(exist_ok=True)\n",
        "        print(f\"üìÅ Created directory: {dir_name}\")\n",
        "    \n",
        "    # Set environment variables for optimization\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
        "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "    os.environ[\"TRANSFORMERS_CACHE\"] = str(TRANSFORMERS_CACHE)\n",
        "    \n",
        "    print(\"‚úÖ Environment setup completed\")\n",
        "    return True\n",
        "\n",
        "def main_installation():\n",
        "    \"\"\"Main installation routine with caching.\"\"\"\n",
        "    print(\"üöÄ Starting Advanced RVC Pro Installation...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Check if installation is cached\n",
        "    if CACHE_FILE.exists():\n",
        "        try:\n",
        "            cache_data = json.loads(CACHE_FILE.read_text())\n",
        "            print(\"üì¶ Installation cache found!\")\n",
        "            print(f\"‚è∞ Previous installation: {cache_data.get('timestamp', 'Unknown')}\")\n",
        "            print(f\"üèóÔ∏è GPU Config: {cache_data.get('gpu_config', 'Unknown')}\")\n",
        "            \n",
        "            # Quick validation\n",
        "            import torch\n",
        "            import gradio as gr\n",
        "            print(\"‚úÖ Cache validation successful - skipping installation!\")\n",
        "            return cache_data.get('gpu_config', {})\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Cache validation failed: {e}\")\n",
        "    \n",
        "    # Perform installation\n",
        "    gpu_config = check_gpu_type()\n",
        "    \n",
        "    success = True\n",
        "    success &= install_pytorch_with_caching()\n",
        "    success &= install_gradio_with_caching()\n",
        "    success &= install_voice_conversion_deps()\n",
        "    success &= setup_environment()\n",
        "    \n",
        "    if success:\n",
        "        # Save cache\n",
        "        cache_data = {\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"gpu_config\": gpu_config,\n",
        "            \"pytorch_version\": subprocess.run([\"python\", \"-c\", \"import torch; print(torch.__version__)\"], \n",
        "                                             capture_output=True, text=True).stdout.strip(),\n",
        "            \"cuda_available\": torch.cuda.is_available() if 'torch' in globals() else False\n",
        "        }\n",
        "        \n",
        "        CACHE_FILE.write_text(json.dumps(cache_data, indent=2))\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"\\nüéâ Installation completed in {elapsed:.1f} seconds!\")\n",
        "        print(f\"üíæ Cache saved to: {CACHE_FILE}\")\n",
        "        return gpu_config\n",
        "    else:\n",
        "        print(\"‚ùå Installation failed!\")\n",
        "        return None\n",
        "\n",
        "# Execute installation\n",
        "GPU_CONFIG = main_installation()\n",
        "\n",
        "if GPU_CONFIG:\n",
        "    print(f\"\\nüéØ GPU Configuration Applied:\")\n",
        "    for key, value in GPU_CONFIG.items():\n",
        "        print(f\"  ‚Ä¢ {key}: {value}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Installation failed - please check the error messages above\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drive_mount"
      },
      "source": [
        "## üóÑÔ∏è Mount Google Drive & Setup Symlinks\n",
        "\n",
        "Keep your models and data persistent across sessions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drive_setup"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# GOOGLE DRIVE MOUNTING & SYMLINK SETUP\n",
        "# ============================================\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"üîó Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "DRIVE_BASE = Path(\"/content/drive/MyDrive\")\n",
        "WORKSPACE = Path.cwd()\n",
        "\n",
        "# Create RVC directory on Drive\n",
        "RVC_DIR = DRIVE_BASE / \"RVC_Models\"\n",
        "RVC_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Setup symlinks for persistent storage\n",
        "symlinks = [\n",
        "    (\"weights\", RVC_DIR / \"weights\"),\n",
        "    (\"indexes\", RVC_DIR / \"indexes\"),\n",
        "    (\"logs\", RVC_DIR / \"logs\"),\n",
        "    (\"cache\", RVC_DIR / \"cache\")\n",
        "]\n",
        "\n",
        "print(\"üîó Setting up symlinks for persistent storage...\")\n",
        "for local_name, drive_path in symlinks:\n",
        "    local_path = WORKSPACE / local_name\n",
        "    \n",
        "    if local_path.exists() and not local_path.is_symlink():\n",
        "        # Backup existing directory\n",
        "        backup_path = local_path.parent / f\"{local_name}_backup\"\n",
        "        local_path.rename(backup_path)\n",
        "        print(f\"üì¶ Backed up {local_name} to {backup_path}\")\n",
        "    \n",
        "    # Create symlink\n",
        "    try:\n",
        "        if local_path.is_symlink():\n",
        "            local_path.unlink()\n",
        "        \n",
        "        # Ensure drive directory exists\n",
        "        drive_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Create symlink\n",
        "        os.symlink(drive_path, local_path)\n",
        "        print(f\"‚úÖ Linked {local_name} -> {drive_path}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not symlink {local_name}: {e}\")\n",
        "\n",
        "print(f\"\\nüíæ Your RVC models will be saved to: {RVC_DIR}\")\n",
        "print(\"üîÑ They will persist across Colab sessions!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "project_setup"
      },
      "source": [
        "## üìÅ Project Setup\n",
        "\n",
        "Clone the repository and setup the project structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# PROJECT CLONING & SETUP\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "# Clone repository\n",
        "REPO_URL = \"https://github.com/ArkanDash/Advanced-RVC-Inference.git\"\n",
        "PROJECT_DIR = Path.cwd() / \"Advanced-RVC-Inference\"\n",
        "\n",
        "if not PROJECT_DIR.exists():\n",
        "    print(\"üì• Cloning Advanced RVC Inference repository...\")\n",
        "    subprocess.run([\"git\", \"clone\", REPO_URL, str(PROJECT_DIR)], check=True)\n",
        "    print(\"‚úÖ Repository cloned successfully\")\n",
        "else:\n",
        "    print(\"üìÅ Repository already exists, updating...\")\n",
        "    os.chdir(PROJECT_DIR)\n",
        "    subprocess.run([\"git\", \"pull\", \"origin\", \"main\"], check=True)\n",
        "    print(\"‚úÖ Repository updated\")\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(PROJECT_DIR)\n",
        "print(f\"üìÇ Working directory: {PROJECT_DIR}\")\n",
        "\n",
        "# Install project in development mode\n",
        "print(\"üîß Installing project in development mode...\")\n",
        "subprocess.run([\"pip\", \"install\", \"-e\", \".\", \"--no-deps\"], check=True)\n",
        "print(\"‚úÖ Project installed successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tunneling"
      },
      "source": [
        "## üåê Setup Tunneling (Choose One)\n",
        "\n",
        "Select your preferred method for accessing the UI externally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tunnel_options"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# TUNNELING OPTIONS\n",
        "# ============================================\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from google.colab import output\n",
        "\n",
        "# Option 1: ngrok (Recommended for stability)\n",
        "def setup_ngrok():\n",
        "    print(\"üåê Setting up ngrok tunnel...\")\n",
        "    \n",
        "    # Install ngrok\n",
        "    try:\n",
        "        subprocess.run([\"wget\", \"https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\"], \n",
        "                      check=True)\n",
        "        subprocess.run([\"tar\", \"xzf\", \"ngrok-v3-stable-linux-amd64.tgz\"], check=True)\n",
        "        subprocess.run([\"chmod\", \"+x\", \"ngrok\"], check=True)\n",
        "        print(\"‚úÖ ngrok installed\")\n",
        "        \n",
        "        # Note: You'll need to add your ngrok auth token\n",
        "        print(\"‚ö†Ô∏è Please add your ngrok auth token:\")\n",
        "        print(\"!./ngrok config add-authtoken YOUR_TOKEN_HERE\")\n",
        "        print(\"\\nThen run:\")\n",
        "        print(\"!./ngrok http 7860\")\n",
        "        \n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Failed to setup ngrok: {e}\")\n",
        "\n",
        "# Option 2: Gradio share (Built-in, less stable)\n",
        "def setup_gradio_share():\n",
        "    print(\"üé® Gradio share is built-in - just add share=True when launching\")\n",
        "    print(\"‚ö†Ô∏è Note: Share links expire after ~72 hours\")\n",
        "\n",
        "# Option 3: LocalTunnel\n",
        "def setup_localtunnel():\n",
        "    print(\"üîó Setting up localtunnel...\")\n",
        "    try:\n",
        "        subprocess.run([\"npm\", \"install\", \"-g\", \"localtunnel\"], check=True)\n",
        "        print(\"‚úÖ localtunnel installed\")\n",
        "        print(\"\\nTo start tunneling:\")\n",
        "        print(\"!npx localtunnel --port 7860\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Failed to setup localtunnel: {e}\")\n",
        "\n",
        "# Display options\n",
        "print(\"Select your tunneling method:\")\n",
        "print(\"1Ô∏è‚É£ ngrok (Most stable, requires auth token)\")\n",
        "print(\"2Ô∏è‚É£ Gradio share (Built-in, less stable)\")\n",
        "print(\"3Ô∏è‚É£ localtunnel (Good alternative)\")\n",
        "print(\"\\nRecommended: ngrok for production use\")\n",
        "\n",
        "# For quick setup, we'll use Gradio share for now\n",
        "USE_GRADIO_SHARE = True\n",
        "USE_NGROK = False\n",
        "USE_LOCALTUNNEL = False\n",
        "\n",
        "if USE_GRADIO_SHARE:\n",
        "    setup_gradio_share()\n",
        "elif USE_NGROK:\n",
        "    setup_ngrok()\n",
        "elif USE_LOCALTUNNEL:\n",
        "    setup_localtunnel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "launch_ui"
      },
      "source": [
        "## üöÄ Launch Advanced RVC Interface\n",
        "\n",
        "Start the web interface with optimized settings for your GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "launch_app"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# LAUNCH ADVANCED RVC APPLICATION\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import logging\n",
        "from pathlib import Path\n",
        "import gradio as gr\n",
        "\n",
        "# Add project to Python path\n",
        "PROJECT_DIR = Path.cwd()\n",
        "sys.path.insert(0, str(PROJECT_DIR))\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# GPU Configuration (from previous cells)\n",
        "if 'GPU_CONFIG' not in globals():\n",
        "    GPU_CONFIG = {\"batch_size\": 4, \"precision\": \"fp16\", \"optimization\": \"balanced\"}\n",
        "\n",
        "print(\"üöÄ Starting Advanced RVC Inference...\")\n",
        "print(f\"üéØ GPU Config: {GPU_CONFIG}\")\n",
        "print(f\"üìÅ Working Directory: {PROJECT_DIR}\")\n",
        "\n",
        "# Launch application with optimized settings\n",
        "try:\n",
        "    # Import the main application\n",
        "    from app import main\n",
        "    \n",
        "    print(\"‚úÖ Application modules loaded successfully\")\n",
        "    \n",
        "    # Launch with Colab-optimized settings\n",
        "    print(\"üåê Launching Web Interface...\")\n",
        "    \n",
        "    # Create a custom launch function for Colab\n",
        "    def launch_colab_app():\n",
        "        import argparse\n",
        "        \n",
        "        # Override sys.argv for Gradio\n",
        "        sys.argv = [\n",
        "            \"app.py\",\n",
        "            \"--port\", \"7860\",\n",
        "            \"--share\", str(USE_GRADIO_SHARE),  # Enable sharing if configured\n",
        "            \"--host\", \"0.0.0.0\",\n",
        "            \"--log-level\", \"INFO\"\n",
        "        ]\n",
        "        \n",
        "        # Configure environment for Colab\n",
        "        os.environ[\"PYTHONUNBUFFERED\"] = \"1\"\n",
        "        \n",
        "        # Launch the app\n",
        "        main()\n",
        "    \n",
        "    # Start in a separate thread for Colab compatibility\n",
        "    import threading\n",
        "    \n",
        "    app_thread = threading.Thread(target=launch_colab_app, daemon=True)\n",
        "    app_thread.start()\n",
        "    \n",
        "    # Give it a moment to start\n",
        "    import time\n",
        "    time.sleep(3)\n",
        "    \n",
        "    print(\"\\nüéâ Application Started Successfully!\")\n",
        "    print(\"üîó Access your interface at:\")\n",
        "    print(\"  ‚Ä¢ Local: http://localhost:7860\")\n",
        "    if USE_GRADIO_SHARE:\n",
        "        print(\"  ‚Ä¢ Public: Check the share link above\")\n",
        "    elif USE_NGROK:\n",
        "        print(\"  ‚Ä¢ ngrok: Check the ngrok output above\")\n",
        "    \n",
        "    print(\"\\nüí° Tips:\")\n",
        "    print(\"  ‚Ä¢ Your models are synced to Google Drive\")\n",
        "    print(\"  ‚Ä¢ Memory usage is optimized for your GPU\")\n",
        "    print(\"  ‚Ä¢ All sessions will persist your trained models\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to launch application: {e}\")\n",
        "    print(\"\\nüîß Troubleshooting:\")\n",
        "    print(\"1. Check if all dependencies are installed\")\n",
        "    print(\"2. Verify GPU is properly detected\")\n",
        "    print(\"3. Check Colab logs for detailed error messages\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
